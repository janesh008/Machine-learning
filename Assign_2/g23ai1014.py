# -*- coding: utf-8 -*-
"""G23AI1014.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15NFX8ELATNz8IWIiIewpeDwzSxNdDg5O

# **ML_Assign_2_Q1**
"""

from google.colab import drive
drive.mount('/content/gdrive')

"""## **Data Preprocess**"""

import pandas as pd
import numpy as np

df = pd.read_csv("/content/diabetes (1).csv")
df.head(100)

print(df.isna().sum())

'''
1.pragnancies does not matter
2. Insulin is not taken because there is lot of missing data in insulin so it's good to drop
so, droping above two columns
'''
# drop_list = [ ]
dfe = ["Pregnancies", "Insulin"]
df = df.drop(dfe, axis =1)
df.head()

import seaborn as sns
import matplotlib.pyplot as plt

corr_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()
# there is no desired correlation in any of the columns so we can't drop any of the columns based on the correlation

"""## **Find optimum prinicipal components**"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

X = df.drop('Outcome', axis=1)

X = StandardScaler().fit_transform(X)   # Standardize the features to have mean=0 and variance=1

# Apply PCA and plot the explained variance ratio for each principal component
pca = PCA().fit(X)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.show()

# so here you see in below plot to reach out .90 varinace number of principal components is 3

"""## **Linear Regression and Random Forest**"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Separate the features and the target
X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Split the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply PCA with the optimum number of components
pca = PCA(n_components=3)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Define the regression models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42)
}

# Train and evaluate the models
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"{name} R2 Score: {r2_score(y_test, y_pred)}")
    print(f"{name} Mean Squared Error: {mean_squared_error(y_test, y_pred)}")

    model.fit(X_train_pca, y_train)
    y_pred_pca = model.predict(X_test_pca)
    print(f"{name} with PCA R2 Score: {r2_score(y_test, y_pred_pca)}")
    print(f"{name} with PCA Mean Squared Error: {mean_squared_error(y_test, y_pred_pca)}")
    print("\\n")

"""**ML_Assign_2_Q2**

# **ML_Assign_2_Q2**

## **Data preprocessing**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

df_train = pd.read_csv("/content/gdrive/MyDrive/Colab Notebooks/ML/ML_Assign_2/fashion-mnist_train.csv")
df_test = pd.read_csv("/content/gdrive/MyDrive/Colab Notebooks/ML/ML_Assign_2/fashion-mnist_test.csv")

# df_test.shape
y_train = df_train.iloc[:, 0]    # y are the labels
X_train = df_train.iloc[:, 1:]   # X is preprocessed and flattened data

# here i choose already flatten data and no need to preprocess the data
df_train.drop(columns=df_train.columns[0], axis=1, inplace=True)
df_train.head(100)

y_test = df_train.iloc[:, 0]    # y are the labels
X_test = df_train.iloc[:, 1:]   # X is preprocessed and flattened data

df_test.drop(columns=df_test.columns[0], axis=1, inplace=True)
df_test.head(100)

"""## **Initialize the kmeans algorithm with 1 data point**"""

'''
a) Train the k-means model on f-MNIST data with k = 10 and 10 random 784 dimensional
points (in input range) as initializations. Report the number of points in each cluster.

-> Here i initialize with 10 random data with 10 clusters
'''
kmeans_1 = KMeans(n_clusters=10, init='random', random_state=0).fit(X_train)
clusters_1 = kmeans_1.labels_  # this shows the predicted label of the data
counts_1 = np.bincount(clusters_1)  # this show the number of points in each clusters

# print(clusters_1)
print(f"number of points in each cluster: {counts_1}")

# b) Visualize the cluster centers of each cluster as 2-d images of all clusters.
centers_1 = kmeans_1.cluster_centers_
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(centers_1[i].reshape(28, 28), cmap='gray')
plt.show()

# c) Visualize 10 images corresponding to each cluster.
for i in range(10):
  p=0
  print(f"for cluster number: {i}")
  for j, k in enumerate(clusters_1):
    # print(j, k)
    if k == i:
      plt.subplot(1, 10, p+1)
      plt.imshow(X.values[j,:].reshape(28, 28), cmap='gray')
      p+=1
      if p==10:
        break
  plt.show()

"""## **Initialize the kmeans algorith with 10 data points from each cluster**"""

'''
d) Train another k-means model with 10 images from each class as initializations , report the
number of points in each cluster

-> Here first find the 10 data points from each clusters and after that find the mean of that 10 data points and apply that centroid as
initialization of the each cluster.
'''

# below is method to find the centroid of the 10 data points from each cluster
init_centroids = []
for i in range(10):
    cls_data = X_train[y_train == i]  # Get all data points of this class
    cls_samples = cls_data.sample(n=10, replace=False)
    cls_centroid = cls_samples.mean(axis=0)
    init_centroids.append(cls_centroid)
init_centroids = np.vstack(init_centroids)  # Stack all centroids together


kmeans_10 = KMeans(n_clusters=10, init=init_centroids, n_init=1).fit(X_train)   #apply k-means model
clusters_10 = kmeans_10.labels_
counts_10 = np.bincount(clusters_10)

# print(clusters_10)
print(f"number of points in each cluster: {counts_10}")

# visualize the cluster centers
centers_10 = kmeans_10.cluster_centers_
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(centers_10[i].reshape(28, 28), cmap='gray')
plt.show()

# E) Visualize 10 images corresponding to each cluster
for i in range(10):
  p=0
  print(f"for cluster number: {i}")
  for j, k in enumerate(clusters_10):
    # print(j, k)
    if k == i:
      plt.subplot(1, 10, p+1)
      plt.imshow(X.values[j,:].reshape(28, 28), cmap='gray')
      p+=1
      if p==10:
        break
  plt.show()

"""## **Compare the accuracy of both model 1 datapoint vs 10 datapoints**"""

'''
f) Evaluate Clusters of part a and part d with Sum of Squared Error (SSE) method. Report
the scores and comment on which case is a better clustering.

-> below i get the good accuracy(accuracy is good when the sum of squared error is low) when i select the 1 data point from each cluster as
compared the select the centroid of the 10 data points from each cluster.
'''

from sklearn.metrics import mean_squared_error
# Predict the labels of the test set: y_pred for kmeans_1 (1 image from each class) and calculate SSE
y_pred_1 = kmeans_1.predict(X_test)
SSE_1 = mean_squared_error(y_test, y_pred_1)

# Predict the labels of the test set: y_pred for kmeans_10 (10 image from each class) and calculate SSE
y_pred_10 = kmeans_10.predict(X_test)
SSE_10 = mean_squared_error(y_test, y_pred_10)

SSE_10 = kmeans_10.inertia_
print(SSE_1, SSE_10)
if SSE_1 < SSE_10:
  print("sse1 is good.")
else:
  print("sse10 is good")

"""# **ML_Assign_2_Q3**

## **Data preprocessing**
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")

df = pd.read_excel("/content/Dry_Bean_Dataset.xlsx")
df.head()

df.shape

df['Class'].unique()

req_col = ["Area", "Perimeter", "MajorAxisLength", "MinorAxisLength", "Eccentricity", "roundness", "AspectRation", "ConvexArea", "Class"]

df_ = df[req_col]
df_.head()

df_.info()

df_.describe()

df_.isnull().values.sum()

df_.isna().values.sum()

import seaborn as sns
import matplotlib.pyplot as plt

corr_matrix = df_.corr()
plt.figure(figsize=(10,8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.show()

'''
here we can see that area is highly correlated with Perimeter, MajorAxisLength, ConvexArea and MinorAxisLength
and aspectration is also highly correlated with the Eccentricity
'''

Remove_corr_col = ["Perimeter", "MajorAxisLength", "MinorAxisLength", "AspectRation", "ConvexArea"]
df_1=df_.drop(Remove_corr_col, axis=1)
df_1.head()

X = df_1.iloc[:,:-1]
y = df_1.iloc[:,-1:]
# df_1.iloc[:,-1:]

num_col = ["Area", "Eccentricity",	"roundness"]
for col in num_col:
    Q1 = df_1[col].quantile(0.25)
    Q3 = df_1[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df_1[(df_1[col] < lower_bound) | (df_1[col] > upper_bound)]
outliers

figure, axis = plt.subplots(2, 2)
axis[0,0].bar(df_1['Class'], df_1['Area'])
axis[0,0].set_title('Class & Area')
axis[0,0].tick_params(axis='x', labelrotation = 100)
# ax.
# axis[0,0].ylabel('Area')

axis[0,1].bar(df_1['Class'], df_1['Eccentricity'])
axis[0,1].set_title('Eccentricity & Class')
axis[0,1].tick_params(axis='x', labelrotation = 100)
# axis[0,1].ylabel('Area')

axis[1,0].bar(df_1['Class'], df_1['roundness'])
axis[1,0].set_title('roundness & Class')
axis[1,0].tick_params(axis='x', labelrotation = 100)
# axis[1,0].ylabel('Area')
figure.tight_layout(pad=5.0)
figure.set_figwidth(10)
figure.set_figheight(15)
# plt.ylim([0.5, 1])
# plt.legend(loc='lower right')

'''
 here 91 outliers and also wants to scale this features(because here all features are not in same scale.
 if we don't scale the feature then weights are going to more fluctuate hence it's affect the gradient descent.
 so scaling helps gradient descent smoothly converge to minima )
 so, removing outliers or replacing by median is not suffiecient way to get
 efficient output so here I am using robust scaler that scale the feature by using median

 and also encode the label feature (that helps neural network to understand different classes)
'''
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import OrdinalEncoder

scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)

encoder = OrdinalEncoder()
y_enc = encoder.fit_transform(y)

"""## **ML modelling for first**"""

from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
import tensorflow as tf
import torch
import torch.nn as nn
import torch.optim as optim

# Here I use the stratify attribute in train_test_split function because to maintain the consistency in the data splitting process.
# Create train, validation, and test splits
x_train, x_temp, y_train, y_temp = train_test_split(X_scaled, y_enc, stratify=y, test_size=0.3)
x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, stratify=y_temp, test_size=0.5)

# Convert the numpy array to PyTorch tensors
X_train = torch.tensor(x_train, dtype=torch.float32)
Y_train = torch.tensor(y_train, dtype=torch.float32)
X_val = torch.tensor(x_val, dtype=torch.float32)
Y_val = torch.tensor(y_val, dtype=torch.float32)
X_test = torch.tensor(x_test, dtype=torch.float32)
Y_test = torch.tensor(y_test, dtype=torch.float32)

'''
b) Implement a multi-layer perceptron
-> here i tried different activation function combination but below combination of activation function works well
here i started with 2 hidden layers because there only 3 feature. so, 2 layers is sufficient to find the pattern in the data. but here below you can
see i get the same accuracy as in 2 hidden layer
'''
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(len(set(Y_train)), activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print('\nTest accuracy:', test_acc)

# Plotting the accuracy
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

"""**below i tried with different combination of parameter in neural network**"""

# b) Implement a multi-layer perceptron
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(len(set(Y_train)), activation = 'tanh')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print('\nTest accuracy:', test_acc)

# Plotting the accuracy
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

# b) Implement a multi-layer perceptron
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='tanh'),
    tf.keras.layers.Dense(len(set(Y_train)), activation='softmax')
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print('\nTest accuracy:', test_acc)

# Plotting the accuracy
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(len(set(Y_train)), activation='softmax')
])
'''
parameter that affects the neural network
learning rate
batch size
regularisation techniques
optimizer algorithm, stochastic gradient descent
xavier initializer
network architectre
data preprocessing
'''
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))

test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print('\nTest accuracy:', test_acc)

# Plotting the accuracy
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0.5, 1])
plt.legend(loc='lower right')

"""## **Activation Function**"""

'''
# c) Now experiment with different activation functions (at least 3) and comment (in the
# report) on how the accuracy varies. Create plots to support your arguments.

# Here I used relu, sigmoid and tanh as activation function for all layers but i find that if we use the 1 hidden layer then sigmoid activation function's
accuracy is good as compared to the relu and tanh
'''
act_fun = ['relu', 'sigmoid', 'tanh']
acc_act = []

# with one hidden layers
for activation in act_fun:
    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(64, activation=activation, input_shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(len(set(Y_train)), activation=activation)
    ])

    # Compile, train, and evaluate the model as before
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
    print('\nTest accuracy:', test_acc)
    acc_act.append(test_acc)

plt.figure(figsize=(12, 6))
plt.bar(act_fun, acc_act)
plt.xlabel('Activation Function')
plt.ylabel('Accuracy')
plt.title('Accuracy for Different Activation Functions')
plt.show()

# with two hidden layer

act_fun = ['relu', 'sigmoid', 'tanh']
acc_act = []

for activation in act_fun:
    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(64, activation=activation, input_shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(64, activation=activation),
        tf.keras.layers.Dense(len(set(Y_train)), activation=activation)
    ])

    # Compile, train, and evaluate the model as before
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
    print('\nTest accuracy:', test_acc)
    acc_act.append(test_acc)

plt.figure(figsize=(12, 6))
plt.bar(act_fun, acc_act)
plt.xlabel('Activation Function')
plt.ylabel('Accuracy')
plt.title('Accuracy for Different Activation Functions')
plt.show()

"""## **Weight Initialization**"""

'''
d) Experiment with different weight initialization: Random, Zero & Constant. Create plots
to support your arguments.

-> here i get very low accuracy when I initialize the weights with zeros because if all the weights are zero then no matter how many nodes output
at the second layers might slight change this will same happen when we initialize the weights with ones. in both this case zeros and ones neural
network not efficiently find the pattern in the data
but i get very good accuracy when I initialize the weights with random number so, in this scenario output at the all nodes of all hidden layers is different
so, the neural network finds pattern in the data
'''
initializers = ['zeros', 'ones', 'random_normal']
acc_wgt_int = []

for initializer in initializers:
    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(64, activation='sigmoid', kernel_initializer=initializer, input_shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(64, activation='sigmoid', kernel_initializer=initializer),
        tf.keras.layers.Dense(len(set(Y_train)), activation='softmax')
    ])

    # Compile, train, and evaluate the model as before
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
    print('\nTest accuracy:', test_acc)
    acc_wgt_int.append(test_acc)

plt.figure(figsize=(12, 6))
plt.bar(initializers, acc_wgt_int)
plt.xlabel('Initializers')
plt.ylabel('Accuracy')
plt.title('Accuracy for Different initializers')
plt.show()

"""## **Hidden nodes**"""

'''
e) Change the number of hidden nodes and comment upon the training and accuracy. Create
plots to support your arguments.

-> here i choose different number of nodes in the hidden layer but as i see in the plot that if network has few nodes than accuracy of the model is
less means underfitting and while increasing number of nodes in the hidden layer then accuracy is increaing but after certain point accuracy is saturated means
when number of nodes increaing then there is chance that model is overfitted so here best accuracy we get at the 32 or number of nodes in the hidden layer
'''

hidden_nodes = [5, 10, 15, 20, 25, 32, 64, 128]
acc_hid_nodes = []

for nodes in hidden_nodes:
    model = tf.keras.models.Sequential([
        tf.keras.layers.Dense(nodes, activation='relu', input_shape=(X_train.shape[1],)),
        tf.keras.layers.Dense(nodes, activation='relu'),
        tf.keras.layers.Dense(len(set(Y_train)), activation='softmax')
    ])

    # Compile, train, and evaluate the model as before
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))

    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
    print('\nTest accuracy:', test_acc)
    acc_hid_nodes.append(test_acc)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(hidden_nodes, acc_hid_nodes, marker='o')
plt.xlabel('Number of Hidden Nodes')
plt.ylabel('Accuracy')
plt.title('Accuracy for Different Numbers of Hidden Nodes')
plt.show()