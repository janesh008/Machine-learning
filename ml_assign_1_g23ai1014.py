# -*- coding: utf-8 -*-
"""ML_Assign_1_G23AI1014.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rdr_WST_WBoVrUxRZhokOcOiwSvkAIXJ

# **ML_Assign_1 _Q1**
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler

df = pd.read_excel("/content/Cars93.xlsx")

l=[]
for i in df.columns:
  if df[i].isnull().values.sum()!=0:
    for j in range(92):
      if df[i].isnull().values[j]==True:
        # print(i, j, df[i][j])
        l.append(j)

print(l) # list of row no. that have nan value

df.head()

# 1. ordinal/nominal/ratio/interval

df["Model"] = df["Model"].astype("category")      # Nominal
df["Type"] = df["Type"].astype("category")        # Nominal
df["Max.Price"] = df["Max.Price"].astype("float") # Interval
df["AirBags"] = df["AirBags"].astype("category")  # Ratio

# 2. remove null and nan value
df.dropna(subset=['Rear.seat.room'], inplace=True)
round_Luggage_room = round(df['Luggage.room'].mean())
df['Luggage.room']=df['Luggage.room'].replace(np.nan, round_Luggage_room)

num_col = ['Min.Price', 'Price', 'Max.Price', 'MPG.city', 'MPG.highway', 'Cylinders', 'EngineSize', 'Horsepower',
           'RPM', 'Rev.per.mile',
           'Fuel.tank.capacity','Passengers', 'Length', 'Wheelbase', 'Width', 'Turn.circle', 'Rear.seat.room',
           'Luggage.room', 'Weight']

for col in num_col:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
    df.loc[outliers.index, col] = df[col].median()

# print(outliers)

from sklearn.preprocessing import OneHotEncoder

cate_col = ['Manufacturer', 'Type', 'AirBags', 'DriveTrain', 'Man.trans.avail',
            'Origin']  # here i remove the Model feature because it have 91 unique value it's almost similar to the row in the dataframe so no need to categorized this feature

df_copy = df.copy()

ohe = OneHotEncoder(sparse=False)

ohe.fit(df_copy[cate_col])

encoded_col = ohe.transform(df_copy[cate_col])

encoded_df = pd.DataFrame(encoded_col, columns=ohe.get_feature_names_out(cate_col))

df_encoded = pd.concat([df_copy.drop(columns=cate_col), encoded_df], axis=1)

df_encoded.head()

# Here Price is mean value of Min.Price and Max.Price so we can drop both of them you can see below
x= round((df['Min.Price'] +df['Max.Price'])/2, 1) -df['Price']
for i in x:
  if i!=0:
    print(i)

df=df.drop(['Min.Price', 'Max.Price'], axis=1)

num_col1 = ['Price', 'MPG.city', 'MPG.highway', 'Cylinders', 'EngineSize', 'Horsepower',
           'RPM', 'Rev.per.mile',
           'Fuel.tank.capacity','Passengers', 'Length', 'Wheelbase', 'Width', 'Turn.circle', 'Rear.seat.room',
           'Luggage.room', 'Weight']

from sklearn.preprocessing import StandardScaler

df_copy = df.copy()
# here we don't have any outliers in the data as we have calculate above so we use here standardscaler
# becuase if we want to preserve the outlier then we can use MinMaxScaler otherwise we can use StandardScaler

ssj = StandardScaler()
df_copy[num_col1] = ssj.fit_transform(df_copy[num_col1])

df_copy.head()

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(df.drop("Price", axis=1), df["Price"], test_size=0.1, random_state=42)

X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2, random_state=42)

print(X_train.shape,  X_val.shape, X_test.shape)

"""# **ML_Assign_1_Q2a**"""

import pandas as pd
from sklearn.linear_model import LinearRegression
import seaborn as sb
import numpy as np
import matplotlib.pyplot as plt

dt = pd.read_excel("/content/linear_regression_dataset.xlsx")

dt.head()

# Linear regression by using inbuilt function
X = dt["Height"].values.reshape(-1, 1)
Y = dt["Weight"].values

x = np.linspace(X.min(), X.max(), 100).reshape(-1,1)

reg = LinearRegression().fit(X, Y)
print(reg.score(X,Y))  #R-squared score
print(reg.coef_)  #slop
print(reg.intercept_)  #Intercept

y_with = reg.predict(x)

plt.plot(x, y_with, color='green', label='with Inbuilt function')
plt.scatter(X, Y, s=32, alpha=.8)

plt.xlabel('Height')
plt.ylabel('Weight')
plt.legend()
plt.show()

# Linear regression without using inbuilt function
mean_X = np.mean(X)
mean_Y = np.mean(Y)

n = len(X)
numerator = 0
denominator = 0

for i in range(n):
  numerator += (X[i] - mean_X) * (Y[i] - mean_Y)
  denominator += (X[i] - mean_X) ** 2

m = numerator / denominator
c = mean_Y - (m * mean_X)

sst = 0
ssr = 0

for i in range(n):
  y_pred = m * X[i] + c
  sst += (Y[i] - mean_Y) ** 2
  ssr += (Y[i] - y_pred) ** 2

r2 = 1 - (ssr/sst)

print(f'm ={m} \nc = {c}')
print(f'R2 error = {r2}')

y_without = m*x + c

plt.plot(x, y_without, color= 'red', label = 'without inbuilt function')
plt.plot(x, y_with, color='green', label='with Inbuilt function')
plt.scatter(X, Y, s=32, alpha=.8)


plt.xlabel('Height')
plt.ylabel('Weight')
plt.legend()
plt.show()

"""# **ML_Assign_1 _Q2b**"""

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt

dt = pd.read_excel("/content/logistic_regression_dataset.xlsx")

dt['Gender'].replace(['Female','Male'], [0,1], inplace = True)
dt.head()

# 1. Split the dataset into training set and test set in the ratio of 70:30 or 80:20
X_train, X_test, Y_train, Y_test = train_test_split(dt.drop("Purchased", axis=1), dt["Purchased"], test_size=0.2, random_state=42)

print(X_train.shape, X_test.shape)

# Train the logistic regression classifier (using inbuilt function)

model = LogisticRegression(solver='liblinear', multi_class='auto', max_iter=1000)
# solver = {lbfgs', 'sag', 'saga', 'liblinear', 'newton-cholesky', 'newton-cg'}
# multi_class = {'multinomial', 'ovr', 'auto'}
model.fit(X_train, Y_train)

y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)

accuracy = accuracy_score(Y_test, y_pred)
report = classification_report(Y_test, y_pred)

cm = confusion_matrix(Y_test, y_pred)

# print(y_pred)
# print(y_prob)
print("Accuracy:", accuracy)
print("Classification report: ")
print(report)
print("Confusion Matrix:\n", cm)

# Train the logistic regression classifier (using inbuilt function)

model = LogisticRegression(solver='newton-cholesky', multi_class='auto', max_iter=1000)
# solver = {lbfgs', 'sag', 'saga', 'liblinear', 'newton-cholesky', 'newton-cg'}
# multi_class = {'multinomial', 'ovr', 'auto'}
model.fit(X_train, Y_train)

y_pred = model.predict(X_test)
y_prob = model.predict_proba(X_test)

accuracy = accuracy_score(Y_test, y_pred)
report = classification_report(Y_test, y_pred)

cm = confusion_matrix(Y_test, y_pred)

# print(y_pred)
# print(y_prob)
print("Accuracy:", accuracy)
print("Classification report: ")
print(report)
print("Confusion Matrix:\n", cm)

"""# **ML_Assign_1 _Q3**"""

# 1. Store the dataset in your google drive and in Colab file load the dataset from your drive.
# 2. Check the shape and head of the dataset.

import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.svm import LinearSVC, SVC

dt = pd.read_excel("/content/SVM.xlsx")

print(dt.shape)
dt.head()

# 3. Age, Experience, Income, CCAvg, Mortgage, Securities are the features and Credit Card is your Target Variable.
# i. Take any 3 features from the six features given above.
# ii. Store features and targets into a separate variable.
# iii. Look for missing values in the data, if any, and address them accordingly.
# iv. Plot a 3D scatter plot using Matplotlib.

droped_feature = ['CCAvg', 'Mortgage', 'Securities', 'CreditCard']

X=dt.drop(droped_feature, axis=1)
Y=dt['CreditCard']

print("Sum of null value in the dataset: ",dt.isnull().values.sum())

fig = plt.figure(figsize = (20, 7))
ax = plt.axes(projection ="3d")

# Creating plot
ax.scatter3D(X['Age'], X['Experience'], X['Income'], color = "green")
plt.title("simple 3D scatter plot")

# show plot
plt.show()

# 4. Split the dataset into 80:20. (3 features and 1 target variable).
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print(X_train.shape, X_test.shape)

# Train the model using scikit learn SVM API (LinearSVC) by setting the regularization
# parameter C as C = {0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000}.
# i. For each value of C, print the score on test data.
# ii. Make the prediction on test data.
# iii. Print the confusion matrix and classification report.

C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]

for i in C:
  model = LinearSVC(C=i, random_state=42, max_iter=10000)
  model.fit(X_train, Y_train)
  classifier_predication = model.predict(X_test)
  print(f"Accuracy for C={i}: ", accuracy_score(Y_test, classifier_predication)*100)

  print("Confusion Matrix: ")
  print(confusion_matrix(Y_test, classifier_predication))

  print("Classification report: ")
  print(classification_report(Y_test, classifier_predication))

# 6. Use gridSearchCV - a cross-validation technique to find the best regularization parameters
# (i.e.: the best value of C).

param_grid = {'C': [0.1, 1, 10, 100, 1000],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['rbf']}

grid_search = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)

grid_search.fit(X_train, Y_train)

print("Best C value: ", grid_search.best_params_["C"])
print("Best score: ", grid_search.best_score_)

y_pred = grid_search.predict(X_test)

print("Confusion Matrix: ")
print(confusion_matrix(Y_test, y_pred))

print("Classification report: ")
print(classification_report(Y_test, y_pred))

# 6. Use gridSearchCV - a cross-validation technique to find the best regularization parameters
# (i.e.: the best value of C).

param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1, 10],
              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],
              'kernel': ['linear']}

grid_search = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)

grid_search.fit(X_train, Y_train)

print("Best C value: ", grid_search.best_params_["C"])
print("Best score: ", grid_search.best_score_)

y_pred = grid_search.predict(X_test)

print("Confusion Matrix: ")
print(confusion_matrix(Y_test, y_pred))

print("Classification report: ")
print(classification_report(Y_test, y_pred))

"""# **ML_Assign_1_Q4**"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris

# Load IRIS dataset
iris = load_iris()

# Create a DataFrame from the dataset
iris_dt = pd.Dat4aFrame(data=iris.data, columns=iris.feature_names)
iris_dt['species'] = iris.target_names[iris.target]


# 1. Visualize the distribution of each feature and the class distribution.
# Plot feature distributions
plt.figure(figsize=(12, 6))
for i, feature in enumerate(iris.feature_names,start=1):
    plt.subplot(2, 2, i)
    sns.histplot(data=iris_dt, x=feature, hue='species', kde=True, bins=20)
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()

# Plot class distribution
plt.figure(figsize=(6, 4))
sns.countplot(data=iris_dt, x='species')
plt.title('Class Distribution')
plt.show()

iris_dt.head()

# 2. Encode the categorical target variable (species) into numerical values.
label_encod = LabelEncoder()
iris_dt['species_encoded'] = label_encod.fit_transform(iris_dt['species'])
iris_dt.head()

# 3. Split the dataset into training and testing sets (use an appropriate ratio).
X = iris_dt.drop(columns=['species', 'species_encoded'])
Y = iris_dt['species_encoded']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print(X_train.shape, X_test.shape)

# 4. Decision Tree Model
# i. Build a decision tree classifier using the training set.
# ii. Visualize the resulting decision tree.
# iii. Make predictions on the testing set and evaluate the model's performance using appropriate metrics (e.g., accuracy, confusion matrix).

dt_clsfr = DecisionTreeClassifier(random_state = 42, max_depth=3, max_leaf_nodes=4)
dt_clsfr.fit(X_train, Y_train)

plt.figure(figsize = (10,6))
plot_tree(dt_clsfr, feature_names=iris.feature_names, class_names = iris.target_names)
plt.show()

Y_pred_test = dt_clsfr.predict(X_test)

# Evaluate performance
test_accuracy = accuracy_score(Y_test, Y_pred_test)
test_conf_matrix = confusion_matrix(Y_test, Y_pred_test)
test_classification_report = classification_report(Y_test, Y_pred_test, target_names=iris.target_names)

print("Decision Tree Accuracy:", test_accuracy)
print("Confusion Matrix:\n", test_conf_matrix)
print("Classification Report:\n", test_classification_report)

# 5. Random Forest Model
# i. Build a random forest classifier using the training set.
# ii. Tune the hyperparameters (e.g., number of trees, maximum depth) if necessary.
# iii. Make predictions on the testing set and evaluate the model's performance using appropriate metrics and compare it with the decision tree model.

rf_classifier = RandomForestClassifier(n_estimators=10, random_state=42)
rf_classifier.fit(X_train, Y_train)

y_pred_rf = rf_classifier.predict(X_test)

# Evaluate performance
accuracy_rf = accuracy_score(Y_test, y_pred_rf)
conf_matrix_rf = confusion_matrix(Y_test, y_pred_rf)
classification_report_rf = classification_report(Y_test, y_pred_rf, target_names=iris.target_names)

print(f"Random Forest Accuracy: ", accuracy_rf)
print("Confusion Matrix:\n", conf_matrix_rf)
print("Classification Report:\n", classification_report_rf)

l =